{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport warnings\n\nwarnings.filterwarnings('ignore')\npd.options.display.float_format = '{:,.3f}'.format\nSEED = 13","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-21T15:24:58.512681Z","iopub.execute_input":"2022-05-21T15:24:58.513031Z","iopub.status.idle":"2022-05-21T15:24:58.518963Z","shell.execute_reply.started":"2022-05-21T15:24:58.512994Z","shell.execute_reply":"2022-05-21T15:24:58.51818Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load Data","metadata":{}},{"cell_type":"code","source":"train_df = pd.read_csv('/kaggle/input/spaceship-titanic/train.csv')\ntest_df = pd.read_csv('/kaggle/input/spaceship-titanic/test.csv')\n\n# Divide inputs and outputs for both training and testing\nX_train = train_df.drop('Transported', axis=1)\ny_train = train_df['Transported'].copy()\n\nX_test = test_df.copy()\n\nXs = [X_train, X_test]","metadata":{"execution":{"iopub.status.busy":"2022-05-21T15:29:18.258502Z","iopub.execute_input":"2022-05-21T15:29:18.25934Z","iopub.status.idle":"2022-05-21T15:29:18.323434Z","shell.execute_reply.started":"2022-05-21T15:29:18.25928Z","shell.execute_reply":"2022-05-21T15:29:18.322502Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Preprocess Data","metadata":{}},{"cell_type":"markdown","source":"### Fill in missing values","metadata":{}},{"cell_type":"code","source":"def fill_journey(df):\n    \"\"\"\n    Fills nulls in 'HomePlanet' and 'Destination' as below:\n    \n    Home Planet:\n    a. Fill with group's most common 'HomePlanet'\n    b. If all group members have null values, it's filled with mode\n    \n    Destination:\n    a. Fills with group's most common 'Destination'\n    b. If all group members have null values, it's filled with mode\n    \"\"\"\n#     home_nan = df[df['HomePlanet'].isnull()].index.tolist()\n#     dest_nan = df[df['Destination'].isnull()].index.tolist()\n#     display(df.iloc[home_nan + dest_nan])\n\n    # Get most common HomePlanet and Destination for everyone\n    most_common_home_planet = df['HomePlanet'].mode().values[0]\n    most_common_dest = df['Destination'].mode().values[0]\n    \n    # Get family most common journey\n    journey = df.fillna(value={'HomePlanet': most_common_home_planet, 'Destination': most_common_dest})\n    journey = journey.groupby(['Group'], as_index=False)['Group', 'HomePlanet', 'Destination'].agg(pd.Series.mode).reindex(columns=df.columns)\n    \n    journey['HomePlanet'] = [el[0] if not isinstance(el, str) and el.size > 1 else el for el in journey['HomePlanet']]\n    journey['Destination'] = [el[0] if not isinstance(el, str) and el.size > 1 else el for el in journey['Destination']]\n    \n    # Fill in null values\n    df = df.merge(journey, how='left', left_on='Group', right_on= 'Group', suffixes= ('', '_'))\n    df['HomePlanet'] = df['HomePlanet'].fillna(df.HomePlanet_);\n    df['Destination'] = df['Destination'].fillna(df.Destination_);\n    \n#     display(df.iloc[home_nan + dest_nan])\n    \n    return df[['HomePlanet', 'Destination']]","metadata":{"execution":{"iopub.status.busy":"2022-05-21T14:46:00.117792Z","iopub.execute_input":"2022-05-21T14:46:00.118378Z","iopub.status.idle":"2022-05-21T14:46:00.130903Z","shell.execute_reply.started":"2022-05-21T14:46:00.118336Z","shell.execute_reply":"2022-05-21T14:46:00.12904Z"},"_kg_hide-input":true,"_kg_hide-output":false,"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def fill_cryosleep(df):\n    \"\"\"\n    Fills nulls in 'CryoSleep' as below:\n    \n    a. For non-spenders, 'CryoSleep' = True as CryoSleep customers don't spend\n    b. For spenders, 'CryoSleep' = False\n    \"\"\"    \n#     nan = df[df['CryoSleep'].isnull()].index.tolist()\n#     display(df.iloc[nan])\n    \n    # Calculate 'TotalSpent' by person\n    df['TotalSpent'] = df['RoomService'] + df['FoodCourt'] + df['ShoppingMall'] + df['Spa'] + df['VRDeck']\n    \n    # Fill in null values\n    df.loc[df['CryoSleep'].isna() & df['TotalSpent'] == 0.0, ['CryoSleep']] = True\n    df.loc[df['CryoSleep'].isna() & df['TotalSpent'] > 0.0, ['CryoSleep']] = False\n    \n#     display(df.iloc[nan])\n    \n    return df[['CryoSleep']]","metadata":{"execution":{"iopub.status.busy":"2022-05-21T14:37:33.943479Z","iopub.execute_input":"2022-05-21T14:37:33.944443Z","iopub.status.idle":"2022-05-21T14:37:33.952948Z","shell.execute_reply.started":"2022-05-21T14:37:33.944388Z","shell.execute_reply":"2022-05-21T14:37:33.95224Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def fill_cabin(df):\n    \"\"\"\n    Fills nulls in 'Cabin' as below:\n    \n    a. Fill with group's most common 'Cabin'\n    b. If all group members have null values, it's filled with mode\n    \"\"\"\n#     nan = df[df['Cabin'].isnull()].index.tolist()\n#     display(df.iloc[nan])\n        \n    # Get most common 'Cabin' for everyone\n    most_common_cabin = df['Cabin'].mode().values[0]\n\n    # Get most common family 'Cabin'\n    cabin_per_family = df.fillna(value={'Cabin': most_common_cabin})\n    cabin_per_family = cabin_per_family.dropna(subset=['Group'])\n    cabin_per_family = cabin_per_family.groupby(['Group'], as_index=False)['Group', 'Cabin'].agg(pd.Series.mode).reindex(columns=df.columns)\n    \n    cabin_per_family['Cabin'] = [el[0] if not isinstance(el, str) and el.size > 1 else el for el in cabin_per_family['Cabin']]\n    \n    # Fill in null values\n    df = df.merge(cabin_per_family, how='left', left_on='Group', right_on='Group', suffixes=('', '_'))\n    df['Cabin'] = df['Cabin'].fillna(df.Cabin_);\n    \n#     display(df.iloc[nan])\n    \n    return df[['Cabin']]","metadata":{"execution":{"iopub.status.busy":"2022-05-21T14:37:35.729578Z","iopub.execute_input":"2022-05-21T14:37:35.730132Z","iopub.status.idle":"2022-05-21T14:37:35.739433Z","shell.execute_reply.started":"2022-05-21T14:37:35.730095Z","shell.execute_reply":"2022-05-21T14:37:35.738221Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def fill_VIP(df):\n    \"\"\"\n    Fills nulls in 'VIP' as below:\n    \n    a. Fill with group's most common 'VIP'\n    b. If all group members have null values, it's filled with mode\n    \"\"\"\n#     nan = df[df['VIP'].isnull()].index.tolist()\n#     display(df.iloc[nan])\n    \n    orig_cols = df.columns\n    \n    # Get most common 'VIP' for everyone\n    most_common_VIP = df['VIP'].mode().values[0]\n\n    # Get most common family 'VIP'\n    VIP_per_family = df.fillna(value={'VIP': most_common_VIP})\n    VIP_per_family = VIP_per_family.dropna(subset=['Group'])\n    VIP_per_family = VIP_per_family.groupby(['Group'], as_index=False)['Group', 'VIP'].agg(pd.Series.mode).reindex(columns=df.columns)\n    \n    VIP_per_family['VIP'] = [el[0] if not isinstance(el, bool) and el.size > 1 else el for el in VIP_per_family['VIP']]\n    \n    # Fill in null values\n    df = df.merge(VIP_per_family, how='left', left_on='Group', right_on='Group', suffixes=('', '_'))\n    df['VIP'] = df['VIP'].fillna(df.VIP_);\n    \n#     display(df.iloc[nan])\n\n    return df[['VIP']]","metadata":{"execution":{"iopub.status.busy":"2022-05-21T14:37:37.534325Z","iopub.execute_input":"2022-05-21T14:37:37.535309Z","iopub.status.idle":"2022-05-21T14:37:37.544054Z","shell.execute_reply.started":"2022-05-21T14:37:37.53526Z","shell.execute_reply":"2022-05-21T14:37:37.543088Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Feature engineering","metadata":{}},{"cell_type":"code","source":"# Fill null values for numerical features with mean\nfor X in Xs:\n    X[['RoomService']] = X[['RoomService']].fillna(value=X[['RoomService']].mean());\n    X[['FoodCourt']] = X[['FoodCourt']].fillna(value=X[['FoodCourt']].mean());\n    X[['ShoppingMall']] = X[['ShoppingMall']].fillna(value=X[['ShoppingMall']].mean());\n    X[['Spa']] = X[['Spa']].fillna(value=X[['Spa']].mean());\n    X[['VRDeck']] = X[['VRDeck']].fillna(value=X[['VRDeck']].mean());\n    X[['Age']] = X[['Age']].fillna(value=X[['Age']].mean());\n\n# Getting group. Also add a new feature 'GroupSize'\nfor X in Xs:\n    X['Group'] = X['PassengerId'].apply(lambda x: x.split('_')[0]).astype(int)\n    X['GroupSize'] = X['Group'].map(lambda x: X['Group'].value_counts()[x])\n    \n# Splitting Name in FirstName and LastName\nfor X in Xs:\n    X['FirstName'] = X['Name'].apply(lambda x: x.split(' ')[0] if (str(x)) != 'nan' else x)\n    X['LastName'] = X['Name'].apply(lambda x: x.split(' ')[1] if (str(x)) != 'nan' else x)\n\n# Adding new feature 'WithGroup'\nfor X in Xs:\n    X['WithGroup'] = 1\n    X.loc[X['GroupSize'] == 1, 'WithGroup'] = 0\n\nfor X in Xs:\n    X[['HomePlanet', 'Destination']] = fill_journey(X)\n    X[['CryoSleep']] = fill_cryosleep(X)\n    X[['VIP']] = fill_VIP(X)\n    X[['Cabin']] = fill_cabin(X)\n    \n# Splitting Cabin into Deck, Num, and Side\nfor X in Xs:\n    X['Deck'] = X['Cabin'].apply(lambda x: x.split('/')[0])\n    X['Num'] = X['Cabin'].apply(lambda x: int(x.split('/')[1]))\n    X['Side'] = X['Cabin'].apply(lambda x: x.split('/')[2])\n\n# Drop redundant features\nfor X in Xs:\n    X.drop(['PassengerId', 'Cabin', 'FirstName', 'Name', 'LastName'], axis=1, inplace=True)\n#     X = X[X['LastName'].notna()]\n    \ndisplay(X_train.head(5))","metadata":{"execution":{"iopub.status.busy":"2022-05-21T14:54:05.607968Z","iopub.execute_input":"2022-05-21T14:54:05.60832Z","iopub.status.idle":"2022-05-21T14:54:23.084049Z","shell.execute_reply.started":"2022-05-21T14:54:05.608285Z","shell.execute_reply":"2022-05-21T14:54:23.083103Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for X in Xs:\n    # Splitting Cabin into Deck, Num, and Side\n    X[['Deck', 'Num', 'Side']] = X['Cabin'].str.split('/', expand=True)\n    \n    # Getting group. Also add a new feature 'GroupSize'\n    X['Group'] = X['PassengerId'].apply(lambda x: x.split('_')[0]).astype(int)\n    X['GroupSize'] = X['Group'].map(lambda x: X['Group'].value_counts()[x])\n    X.loc[X['GroupSize'] == 1, 'WithGroup'] = 0\n    \n    # Fill nan values for numerical features with their mean\n    X['RoomService'] = X[['RoomService']].fillna(value=X[['RoomService']].mean());\n    X['FoodCourt'] = X[['FoodCourt']].fillna(value=X[['FoodCourt']].mean());\n    X['ShoppingMall'] = X[['ShoppingMall']].fillna(value=X[['ShoppingMall']].mean());\n    X['Spa'] = X[['Spa']].fillna(value=X[['Spa']].mean());\n    X['VRDeck'] = X[['VRDeck']].fillna(value=X[['VRDeck']].mean());\n    X['Age'] = X[['Age']].fillna(value=X[['Age']].mean());\n    X['Num'] = X['Num'].astype(float)\n    X['Num']= X['Num'].fillna(X['Num'].mean())\n\n    # Fill nan values for categorical features with their mode\n    X['HomePlanet'] = X['HomePlanet'].fillna(X['HomePlanet'].mode())\n    X['Destination'] = X['Destination'].fillna(X['Destination'].mode())\n    X['CryoSleep'] = X['CryoSleep'].fillna(X['CryoSleep'].mode())\n    X['VIP'] = X['VIP'].fillna(X['VIP'].mode())\n    X['Deck']= X['Deck'].fillna(X['Deck'].mode())\n    X['Side']= X['Side'].fillna(X['Side'].mode())\n\n    # Drop redundant features\n    X.drop(['PassengerId', 'Cabin', 'Name'], axis=1, inplace=True)\n    \ndisplay(X_train.head(5))","metadata":{"execution":{"iopub.status.busy":"2022-05-21T15:29:21.045696Z","iopub.execute_input":"2022-05-21T15:29:21.04608Z","iopub.status.idle":"2022-05-21T15:29:30.439503Z","shell.execute_reply.started":"2022-05-21T15:29:21.046041Z","shell.execute_reply":"2022-05-21T15:29:30.438583Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Normalization","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import LabelEncoder\n\nscaler = StandardScaler()\nlabel_enc = LabelEncoder()\n\n# Get categorical features\ncateg_cols = ['CryoSleep', 'VIP', 'Side', 'WithGroup']\n\n# Encode categorical features\nfor X in Xs:\n    for i in categ_cols:\n        X[i] = label_enc.fit_transform(X[i])\n\n# Encode output\ny_train = label_enc.fit_transform(y_train)\n\n# Get numerical columns\nnumerical_cols = ['Age', 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck', 'Group', 'GroupSize', 'Num']\n\n# Normalize numerical columns\nX_train[numerical_cols] = scaler.fit_transform(X_train[numerical_cols])\nX_test[numerical_cols] = scaler.transform(X_test[numerical_cols])\n\n# One hot encoding for rest of categorical columns (non-ordered columns)\nto_onehot = ['HomePlanet', 'Destination', 'Deck']\n\nX_train = pd.get_dummies(X_train, columns=to_onehot)\nX_test = pd.get_dummies(X_test, columns=to_onehot)\n\nX_train.head(5)","metadata":{"execution":{"iopub.status.busy":"2022-05-21T15:29:32.790778Z","iopub.execute_input":"2022-05-21T15:29:32.791095Z","iopub.status.idle":"2022-05-21T15:29:32.865457Z","shell.execute_reply.started":"2022-05-21T15:29:32.791062Z","shell.execute_reply":"2022-05-21T15:29:32.864512Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training","metadata":{}},{"cell_type":"markdown","source":"### Finding best hyperparameters for LogisticRegression","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV\n\nlog_reg_grid = [\n    {\n        'penalty': ['l1'],\n        'solver': ['liblinear', 'saga'],\n        'max_iter': [100, 500, 1000, 1500]\n    },\n    {\n        'penalty': ['l2'],\n        'solver': ['liblinear', 'sag', 'saga', 'newton-cg', 'lbfgs'],\n        'max_iter': [100, 500, 1000, 1500]\n    },\n]\n\nlog_reg_optimal = GridSearchCV(LogisticRegression(), log_reg_grid, scoring = 'accuracy')\nlog_reg_optimal.fit(X_train, y_train)\nprint(log_reg_optimal.best_score_)\nprint(log_reg_optimal.best_params_)\n\n# 0.7883759495453978\n# {'max_iter': 100, 'penalty': 'l1', 'solver': 'saga'}","metadata":{"execution":{"iopub.status.busy":"2022-05-21T15:53:47.648786Z","iopub.execute_input":"2022-05-21T15:53:47.64916Z","iopub.status.idle":"2022-05-21T15:55:24.946352Z","shell.execute_reply.started":"2022-05-21T15:53:47.649108Z","shell.execute_reply":"2022-05-21T15:55:24.945336Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Finding best hyperparameters for SVC","metadata":{}},{"cell_type":"code","source":"from sklearn.svm import SVC\nfrom sklearn.model_selection import GridSearchCV\n\nsvc_grid = [{\n    'C': [0.25, 0.5, 0.75, 1, 1.25, 1.5],\n    'kernel': ['linear', 'poly', 'rbf'],\n    'gamma': ['scale', 'auto']\n}]\n\nsvc_optimal = GridSearchCV(SVC(random_state=SEED), svc_grid, scoring = 'accuracy')\nsvc_optimal.fit(X_train, y_train)\nprint(svc_optimal.best_score_)\nprint(svc_optimal.best_params_)\n\n# 0.7950397454823227\n# {'C': 0.75, 'gamma': 'scale', 'kernel': 'rbf'}","metadata":{"execution":{"iopub.status.busy":"2022-05-21T15:55:53.885356Z","iopub.execute_input":"2022-05-21T15:55:53.885696Z","iopub.status.idle":"2022-05-21T16:02:53.855064Z","shell.execute_reply.started":"2022-05-21T15:55:53.885663Z","shell.execute_reply":"2022-05-21T16:02:53.853967Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Finding best hyperparameters for MLPClassifier","metadata":{}},{"cell_type":"code","source":"from sklearn.neural_network import MLPClassifier\nfrom sklearn.model_selection import GridSearchCV\n\n# mlp_grid = [{\n#     'activation' : ['identity', 'logistic', 'tanh', 'relu'],\n#     'solver' : ['lbfgs', 'sgd', 'adam'],\n#     'hidden_layer_sizes': [\n#         (128,),(256,),(256,128,),(128,64,),(128,64,32,)\n#     ],\n#     'learning_rate': ['constant', 'invscaling', 'adaptive'],\n#     'momentum': [0.9, 0.8, 0.7]\n# }]\n\nmlp_grid = [{\n    'hidden_layer_sizes': [\n        (512,256,128,), (128,64,32,),\n    ],\n    'learning_rate': ['constant', 'invscaling'],\n    'momentum': [0.9, 0.85, 0.8]\n}]\n\nmlp_optimal = GridSearchCV(MLPClassifier(), mlp_grid, cv=3,scoring='accuracy')\nmlp_optimal.fit(X_train, y_train)\nprint(mlp_optimal.best_score_)\nprint(mlp_optimal.best_params_)\n\n# 0.7939751740841658\n# {'activation': 'logistic', 'hidden_layer_sizes': (512, 256, 128), 'learning_rate': 'constant', 'momentum': 0.8, 'solver': 'adam'}\n\n# 0.7562821677263094\n# {'hidden_layer_sizes': (128, 64, 32), 'learning_rate': 'invscaling', 'momentum': 0.8}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Training models with best hyperparameters found","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.svm import SVC\nfrom xgboost import XGBClassifier\n\nMLA = [\n    LogisticRegression(\n        max_iter=500,\n        penalty='l1', \n        solver='saga',\n    ),\n    KNeighborsClassifier(),\n    DecisionTreeClassifier(),\n    RandomForestClassifier(),\n    SVC(\n        C=0.75,\n        gamma='auto',\n        kernel='rbf',\n    ),\n    XGBClassifier(),\n    MLPClassifier(\n        activation='logistic',\n        learning_rate='invscaling',\n        momentum=0.8,\n        hidden_layer_sizes=(128, 64, 32),\n        solver='adam',\n    )\n]\n\n# Setting up the table to compare the performances of each model\nMLA_cols = ['Model', 'Accuracy']\nMLA_compare = pd.DataFrame(columns = MLA_cols)\n\nrow_index = 0\nfor model in MLA:\n    MLA_compare.loc[row_index, 'Model'] = model.__class__.__name__\n    cv_results = cross_val_score(model, X_train, y_train, cv=10, scoring='accuracy')\n    MLA_compare.loc[row_index, 'Accuracy'] = cv_results.mean()\n    \n    row_index += 1\n\nMLA_compare.sort_values(by=['Accuracy'], ascending=False, inplace=True)\nMLA_compare","metadata":{"execution":{"iopub.status.busy":"2022-05-21T16:03:43.38302Z","iopub.execute_input":"2022-05-21T16:03:43.383718Z","iopub.status.idle":"2022-05-21T16:09:51.229906Z","shell.execute_reply.started":"2022-05-21T16:03:43.383669Z","shell.execute_reply":"2022-05-21T16:09:51.228984Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Create ensemble using the best 3 models found","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import VotingClassifier\n\n# Best models found\nmlp_optimal = MLPClassifier(\n    activation='logistic',\n    learning_rate='invscaling',\n    momentum=0.8,\n    hidden_layer_sizes=(128, 64, 32),\n    solver='adam',\n)\nsvc_optimal = SVC(\n    C=0.75,\n    gamma='auto',\n    kernel='rbf',\n)\nlog_reg_optimal = LogisticRegression(\n    max_iter=500,\n    penalty='l1', \n    solver='saga',\n)\n\n# Define ensemble\nhard_ensemble = VotingClassifier(\n    estimators=[\n        ('MLP', mlp_optimal),\n        ('SVC', svc_optimal),\n        ('LogReg', log_reg_optimal)\n    ],\n    voting = 'hard'\n)\n\n# Return accuracy scores\nhard_cross_val = cross_val_score(hard_ensemble, X_train, y_train, scoring='accuracy')\nprint('Hard voting ensemble score:' , hard_cross_val.mean())","metadata":{"execution":{"iopub.status.busy":"2022-05-21T16:18:55.876932Z","iopub.execute_input":"2022-05-21T16:18:55.87728Z","iopub.status.idle":"2022-05-21T16:20:39.81773Z","shell.execute_reply.started":"2022-05-21T16:18:55.877244Z","shell.execute_reply":"2022-05-21T16:20:39.816664Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Submission","metadata":{}},{"cell_type":"code","source":"# Function to predict entries\ndef predict(model):\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    predictions = pd.DataFrame(\n        {\n            'PassengerId': test_df['PassengerId'],\n            'Transported': y_pred\n        }\n    )\n    \n    # Transform to 'True'/'False'\n    predictions[\"Transported\"] = predictions[\"Transported\"].astype(bool)\n    \n    return predictions","metadata":{"execution":{"iopub.status.busy":"2022-05-21T16:35:03.343972Z","iopub.execute_input":"2022-05-21T16:35:03.344333Z","iopub.status.idle":"2022-05-21T16:35:03.351452Z","shell.execute_reply.started":"2022-05-21T16:35:03.3443Z","shell.execute_reply":"2022-05-21T16:35:03.350263Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Submit predictions using best models ","metadata":{}},{"cell_type":"code","source":"predict(svc_optimal).to_csv('submission_svc.csv', index=False)\npredict(mlp_optimal).to_csv('submission_mlp.csv', index=False)\npredict(log_reg_optimal).to_csv('submission_log_reg.csv', index=False)\npredict(hard_ensemble).to_csv('submission_ensemble.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-05-21T16:35:06.418208Z","iopub.execute_input":"2022-05-21T16:35:06.418561Z","iopub.status.idle":"2022-05-21T16:36:23.325992Z","shell.execute_reply.started":"2022-05-21T16:35:06.41852Z","shell.execute_reply":"2022-05-21T16:36:23.325083Z"},"trusted":true},"execution_count":null,"outputs":[]}]}